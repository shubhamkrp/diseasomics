# -*- coding: utf-8 -*-
"""wikipedia2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aRvGMk3-TrgiS9enoStL74Yv-0HFyKFt
"""

import pandas as pd
!pip install pronto

import pronto
ontology1 = pronto.Ontology('/content/symp.obo')
ontology2 = pronto.Ontology('/content/doid.obo')

c1,c2=0,0
for t in ontology1.terms():
  if t.obsolete: c1+=1

for t in ontology2.terms():
  if t.obsolete: c2+=1

print(c1)
print(c2)

!pip install wikipedia-api
import wikipediaapi
wiki = wikipediaapi.Wikipedia('DiseasomicsRep (vaishnavipokkula@gmail.com)', 'en')

# Function to get the "Signs and symptoms" section
def get_signs_and_symptoms(page):
    section_text = ""
    for section in page.sections:
        if section.title.lower() == "signs and symptoms":
            section_text = section.text
            break
    return section_text

# Function to get the "Signs" section
def get_signs(page):
    section_text = ""
    for section in page.sections:
        if section.title.lower() == "signs":
            section_text = section.text
            break
    return section_text

# Function to get the "Symptoms" section
def get_symptoms(page):
    section_text = ""
    for section in page.sections:
        if section.title.lower() == "symptoms":
            section_text = section.text
            break
    return section_text

disease_pgtitle=pd.read_csv('/content/disease_pgtitle.csv')
disease_pgtitle.columns=["id","name","pagetitle","superpagetitle"]
print(disease_pgtitle.head())

print(len(disease_pgtitle))

for t in ontology2.terms():
  if t.obsolete:
    disease_pgtitle=disease_pgtitle[disease_pgtitle['id']!=t.id]

print(len(disease_pgtitle))

print(len(disease_pgtitle)-disease_pgtitle['pagetitle'].isnull().sum())

print(disease_pgtitle['pagetitle'].isnull().sum())

print(len(disease_pgtitle)-disease_pgtitle['superpagetitle'].isnull().sum())

these_ids=[]
for idx, row in disease_pgtitle.iterrows():
  if not pd.isna(row['superpagetitle']):
    these_ids.append(row['id'])


#25 minute runtime
#GETTING IF SIGNS OR SYMPTOMS ARE PRESENT IN DOID PAGETITLES (NOT SUPER PAGETITLES)
no_section_ids=[] #no signs and symptoms section in the wiki page (PAGETITLE)
dis_symp={}
for term in ontology2.terms():
  dis_symp[term.id]=[]

# Iterate over each row in the DataFrame
for idx, row in disease_pgtitle.iterrows():
    page = None
    if row['pagetitle']:
      page = wiki.page(row['pagetitle'])

    if page:
        signs_and_symptoms_text = get_signs_and_symptoms(page)

    if page and signs_and_symptoms_text=="":
      signs_and_symptoms_text=get_signs(page)

    if page and signs_and_symptoms_text=="":
     signs_and_symptoms_text=get_symptoms(page)

    #if page.exists() and signs_and_symptoms_text == "" : # changed it to
    if pd.notna(row['pagetitle']) and signs_and_symptoms_text == "" :
        no_section_ids.append(row['id'])

    if signs_and_symptoms_text != "":
        for term in ontology1.terms():
          if not term.obsolete:
            if term.name in signs_and_symptoms_text:
                dis_symp[row['id']].append(term.id)
            else:
                for synonym in term.synonyms:
                  synonym=synonym.description
                  if str(synonym) in signs_and_symptoms_text:
                    dis_symp[row['id']].append(term.id)

# Display the results
print("IDs with no signs and symptoms section:", len(no_section_ids))

count=0
for doid,values in dis_symp.items():
  if dis_symp[doid]!=[]: count+=1
print(count)
# is the number of diseases which got symptoms


count=0
for doid,values in dis_symp.items():
  if values==[]:
    row=disease_pgtitle[disease_pgtitle['id']==doid]
    if pd.notna(row['pagetitle']).any():
      count+=1

print(count)
# is the number of diseases which had pagetitle but have not got symptoms


print(len(no_section_ids))

sympless_withpage_ids=[]

for doid_,symp_list in dis_symp.items():
  if symp_list==[]:
    row=disease_pgtitle[disease_pgtitle['id'] == doid_]
    if (pd.notna(row['pagetitle'])).any():
      sympless_withpage_ids.append(row['id'].values[0])

print(len(sympless_withpage_ids))

import csv
file_path = 'no_match_ids.csv'

# Write the list to a CSV file
with open(file_path, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows([[id_] for id_ in sympless_withpage_ids])  # Convert list to 2D list for CSV

print(f"CSV file saved as {file_path}")

#FROM HERE

#for no_section_ids, using full text since almost all text is small. (otherwise, should use text summary or UMLS metamap)

still_nomatch_ids=[] #no signs and symptoms section in the wiki page (PAGETITLE)
i=0
for id_ in sympless_withpage_ids:
  i+=1
  wiki_wiki = wikipediaapi.Wikipedia(user_agent='DiseasomicsRep (vaishnavipokkula@gmail.com)', language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)
  if i<3000: #redundant:
    row = disease_pgtitle[disease_pgtitle['id'] == id_]
    title = row.iloc[0]['pagetitle']
    page=wiki_wiki.page(title)

    for term in ontology1.terms():
      if term.name in page.text and not term.obsolete: dis_symp[id_].append(term.id)   # change this to [0:1000]
      else:
        for synonym in term.synonyms:
          synonym=synonym.description
          if str(synonym) in page.text and not term.obsolete: dis_symp[id_].append(term.id) # change this to [0:1000]
    if dis_symp[id_]==[]: still_nomatch_ids.append(id_)

print(len(still_nomatch_ids))
#see runtime above cell: 10m

obs_doid=[]
for term in ontology2.terms():
  if term.obsolete: obs_doid.append(term.id)

for id_ in obs_doid:
  del dis_symp[id_]

print(len(dis_symp))

count=0
for id_, items in dis_symp.items():
   if dis_symp[id_]!=[]:
    count+=1
print(count)

print(len(dis_symp))

print(still_nomatch_ids)

import csv
file_path = 'still_nomatch_ids.csv'

# Write the list to a CSV file
with open(file_path, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows([[id_] for id_ in still_nomatch_ids])  # Convert list to 2D list for CSV

print(f"CSV file saved as {file_path}")

# Write dictionary to CSV
with open('dis_symp_relations.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Disease', 'Symptom'])  # Header row

    # Write each disease and its symptoms
    for disease, symptoms in dis_symp.items():
        for symptom in symptoms:
          #symptoms_str = ', '.join(symptoms)  # Join symptoms with a comma
          writer.writerow([disease, symptom])

print("CSV file 'dis_symp_relations.csv' created with disease-symptom relationships.")

count=0
for id_,symps in dis_symp.items():
  if dis_symp[id_]!=[]:
    count+=1
count

dis_symp_final={}
for id_,symps in dis_symp.items():
  if dis_symp[id_]!=[]:
    count+=1
    dis_symp_final[id_]=dis_symp[id_]
print(len(dis_symp_final))

i=0
for x in dis_symp_final:
  i+=1
  if i<10: print(x)

print(len(these_ids))

print(these_ids[0:7])

count=0
matched_terms = []
for term in ontology2.terms():
    if term.id in these_ids:
        for kind in term.superclasses(distance=1):
          jjj=kind.id
          if jjj in dis_symp_final:
            count+=1
            break
count

# Write dictionary to CSV
with open('final_dis_symp_relations.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Disease_id', 'Symptom_id'])  # Header row

    # Write each disease and its symptoms
    for disease, symptoms in dis_symp_final.items():
        for symptom in symptoms:
          #symptoms_str = ', '.join(symptoms)  # Join symptoms with a comma
          writer.writerow([disease, symptom])

print("CSV file 'final_dis_symp_relations.csv' created with disease-symptom relationships.")
